<!DOCTYPE html>
<html>
<head>
    <title>SC - Research</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 50px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2 {
            color: #333;
        }
        nav {
            background-color: #333;
            padding: 15px;
            margin-bottom: 20px;
        }
        nav ul {
            list-style-type: none;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
        }
        nav ul li {
            margin: 0 20px;
        }
        nav ul li a {
            color: white;
            text-decoration: none;
            font-size: 18px;
        }
        nav ul li a:hover {
            color: #ddd;
        }
        .publications {
            max-width: 800px;
            margin: 0 auto;
            text-align: left;
        }
        .publication {
            margin-bottom: 30px;
        }
        .publication h3 {
            margin: 0;
            font-size: 1.2em;
            color: #2c3e50;
        }
          .publication h4 {
            margin: 0;
            color: #2c3e50;
            margin-bottom: 5px;
        }
        .publication p {
            margin: 5px 0;
            font-size: 0.9em;
            text-align: justify;
        }
        .publication .links a {
            color: #0066cc;
            text-decoration: none;
            margin-right: 10px;
        }
        .publication .links a:hover {
            text-decoration: underline;
        }

        /* ADDED: space around figures and responsive sizing */
        .publications img {
            display: block;          /* ensures margin applies vertically as expected */
            margin: 24px auto;      /* space above & below each figure, centered horizontally */
            max-width: 100%;        /* prevents overflow on small screens */
            height: auto;
            box-sizing: border-box;
        }

        /* Optional: if you want slightly larger gap for paragraphs next to figures */
        .publication p + img,
        img + p {
            margin-top: 28px;
            margin-bottom: 28px;
        }
    </style>
</head>
    
<body>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
        </ul>
    </nav>
    <div class="publications">
        <h1>Artificial Stimulation and Perception</h1>
        <div class="Artificial Stimulation and Perception">
            <h3>Measurement of perceptual distances in Mice</h3>
            
            <p style="text-align: justify;">
                My research asks a simple but fundamental question: how does neural activity give rise to sensory perception, such as sounds and smells?
            </p>
            
            <p style="text-align: justify;">
                Perception of the external world emerges from complex and dynamic patterns of neuronal activity that evolve across multiple dimensions in space and time. To uncover the causal links between neural activity and perception, it is necessary to manipulate sensory inputs while measuring the resulting changes in perceptual decisions. My goal is to identify, during behavior, the spatiotemporal features of brain activity that generate natural sensory representations and to directly compare them with artificial patterns of neural activity created using optogenetics. By doing so, I test—at high throughput level—whether artificial activity introduced into sensory circuits can mimic the structure, dynamics, and perceptual meaning of signals naturally evoked by sensory stimuli.
            </p>
            
            <p style="text-align: justify;">
                This work has implications for sensory prosthetics, neural decoding, and the broader question of how the brain constructs perception.
            </p>
            
            <img src="images/Screenshot 2025-12-08 at 11.58.55 AM.png" alt="Schematic_1" />
            <p style="text-align: justify;">
                To directly compare artificial patterns of neural activity with natural sensory stimuli, 
                I further developed a high-throughput behavioral paradigm that measures perceptual distances between stimuli in mice (Nakayama et al., 2022, Cell Reports). 
                In this delayed match-to-sample task, mice learn to report whether two sequentially presented odorants are the same or different. 
                Perceptual distance between odor pairs is quantified based on performance in non-match trials, providing a behavioral metric of sensory similarity.
            <img src="images/DMTS.png" alt="Schematic_DMTS" width="350" />
                I extended this paradigm to compare artificially generated patterns of neural activity with natural odor-evoked representations within the same behavioral sessions. 
                This extension allows stable and independent assessment of perceptual judgments for both natural and artificial stimuli, enabling direct comparison between the two.
                
            </p>
            
            <img src="images/DMTS_withLight.png" alt="Schematic_2" />
            <p style="text-align: justify;">
                In the same sessions, mice were able to discriminate the similarity of natural odors and evaluate the perceptual distance of artificially induced neural sequences. This demonstrates that animals generalized the decision rule learned with odorants to artificial spatiotemporal patterns, treating them as meaningful sensory inputs rather than novel or irrelevant signals. To test the limits of this paradigm, animals were also challenged to discriminate single activation spots approximately the size of a single glomerulus. Even under these minimal conditions, mice maintained high performance on natural odors and were able to discriminate certain pairs of light spots, demonstrating the sensitivity and resolution of the behavioral paradigm without requiring additional extensive training.
            </p>
           
            <p>
              <!-- updated: link to new page + fragment id -->
              <a href="experimental.html#experimental-setup">Experimental Setup</a>
            </p>
 
        </div>
        <div class="publication">
            <h3>Theme2</h3>
            <p>...</p>
           
        </div>
        
    </div>
</body>
</html>
